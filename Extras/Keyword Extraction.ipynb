{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword extraction with `TfidfVectorizer`\n",
    "\n",
    "Scikit-learn's `CountVectorizer` class creates matrices of word counts and is frequently uses in text-classification tasks. The related [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) class creates matrices of [term freqeuency-inverse document frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (Tf-Idf) values that reflect not just the presence of individual words, but each word's importance. One use for `TfidfVectorizer` is extracting keywords from documents. Let's use it to extract keywords from a book chapter on machine learning. Begin by loading the chapter from a text file and showing the first few paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Software developers are accustomed to solving problems algorithmically. Given a recipe, or algorithm, it's not difficult to write an app that hashes a password or computes a monthly mortgage payment. You code up the algorithm, feed it input, and receive output in return. It's another proposition altogether to write code that determines whether a photo contains a cat or a dog. You can try to do it algorithmically, but the minute you get it working, I'll send you a cat or dog picture that breaks the algorithm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Machine learning takes a different approach to turning input into output. Rather than rely on you to implement an algorithm, it examines a dataset consisting of inputs and outputs and learns how to generate output of its own. Under the hood, special algorithms called learning algorithms build mathematical models of the data and codify the relationship between data going in and data coming out. Once trained in this manner, a model can accept new inputs and generate outputs consistent with the ones in the training data.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To use machine learning to distinguish between cats and dogs, you don't code a cat-vs-dog algorithm. Instead, you train a machine-learning model with cat and dog photos. Success depends on the learning algorithm used and the quality and volume of the training data. Part of becoming a machine-learning engineer is familiarizing yourself with the various learning algorithms and developing an intuition for when to use one versus another. That intuition begins with an examination of machine learning itself.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is Machine Learning?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>At an existential level, machine learning (ML) is a means for finding patterns in numbers and exploiting those patterns to make predictions. Train a model with thousands (or millions) of xs and ys, and let it learn from the data so that given a new x, it can predict what y will be. Learning is the process by which ML finds patterns that can be used to predict future outputs, and it's where the 'learning' in 'machine learning' comes from.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             0\n",
       "0            Software developers are accustomed to solving problems algorithmically. Given a recipe, or algorithm, it's not difficult to write an app that hashes a password or computes a monthly mortgage payment. You code up the algorithm, feed it input, and receive output in return. It's another proposition altogether to write code that determines whether a photo contains a cat or a dog. You can try to do it algorithmically, but the minute you get it working, I'll send you a cat or dog picture that breaks the algorithm.\n",
       "1  Machine learning takes a different approach to turning input into output. Rather than rely on you to implement an algorithm, it examines a dataset consisting of inputs and outputs and learns how to generate output of its own. Under the hood, special algorithms called learning algorithms build mathematical models of the data and codify the relationship between data going in and data coming out. Once trained in this manner, a model can accept new inputs and generate outputs consistent with the ones in the training data.\n",
       "2                  To use machine learning to distinguish between cats and dogs, you don't code a cat-vs-dog algorithm. Instead, you train a machine-learning model with cat and dog photos. Success depends on the learning algorithm used and the quality and volume of the training data. Part of becoming a machine-learning engineer is familiarizing yourself with the various learning algorithms and developing an intuition for when to use one versus another. That intuition begins with an examination of machine learning itself.\n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    What is Machine Learning?\n",
       "4                                                                                    At an existential level, machine learning (ML) is a means for finding patterns in numbers and exploiting those patterns to make predictions. Train a model with thousands (or millions) of xs and ys, and let it learn from the data so that given a new x, it can predict what y will be. Learning is the process by which ML finds patterns that can be used to predict future outputs, and it's where the 'learning' in 'machine learning' comes from."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Data/chapter-1.txt', sep='\\n', header=None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the paragraphs and show the first few lines of the resulting word matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1s 0s</th>\n",
       "      <th>add column</th>\n",
       "      <th>annual income</th>\n",
       "      <th>build mathematical</th>\n",
       "      <th>cat dog</th>\n",
       "      <th>classification models</th>\n",
       "      <th>column contains</th>\n",
       "      <th>credit card</th>\n",
       "      <th>data data</th>\n",
       "      <th>data points</th>\n",
       "      <th>...</th>\n",
       "      <th>sepal width</th>\n",
       "      <th>setosa versicolor</th>\n",
       "      <th>spending score</th>\n",
       "      <th>spending scores</th>\n",
       "      <th>supervised learning</th>\n",
       "      <th>train machine</th>\n",
       "      <th>training data</th>\n",
       "      <th>unsupervised learning</th>\n",
       "      <th>use following</th>\n",
       "      <th>versicolor virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.526645</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.411714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.329181</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.297392</td>\n",
       "      <td>0.257343</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.727929</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.40287</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.670370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1s 0s  add column  annual income  build mathematical   cat dog  \\\n",
       "0  0.000000         0.0            0.0            0.000000  1.000000   \n",
       "1  0.000000         0.0            0.0            0.526645  0.000000   \n",
       "2  0.000000         0.0            0.0            0.000000  0.329181   \n",
       "3  0.000000         0.0            0.0            0.000000  0.000000   \n",
       "4  0.000000         0.0            0.0            0.000000  0.000000   \n",
       "5  0.727929         0.0            0.0            0.000000  0.000000   \n",
       "6  0.000000         0.0            0.0            0.000000  0.000000   \n",
       "7  0.670370         0.0            0.0            0.000000  0.000000   \n",
       "8  0.000000         0.0            0.0            0.000000  0.000000   \n",
       "9  0.000000         0.0            0.0            0.000000  0.000000   \n",
       "\n",
       "   classification models  column contains  credit card  data data  \\\n",
       "0                    0.0          0.00000          0.0        0.0   \n",
       "1                    0.0          0.00000          0.0        0.0   \n",
       "2                    0.0          0.00000          0.0        0.0   \n",
       "3                    0.0          0.00000          0.0        0.0   \n",
       "4                    0.0          0.00000          0.0        0.0   \n",
       "5                    0.0          0.40287          0.0        0.0   \n",
       "6                    0.0          0.00000          0.0        0.0   \n",
       "7                    0.0          0.00000          0.0        0.0   \n",
       "8                    0.0          0.00000          0.0        0.0   \n",
       "9                    0.0          0.00000          0.0        0.0   \n",
       "\n",
       "   data points  ...  sepal width  setosa versicolor  spending score  \\\n",
       "0          0.0  ...          0.0                0.0             0.0   \n",
       "1          0.0  ...          0.0                0.0             0.0   \n",
       "2          0.0  ...          0.0                0.0             0.0   \n",
       "3          0.0  ...          0.0                0.0             0.0   \n",
       "4          0.0  ...          0.0                0.0             0.0   \n",
       "5          0.0  ...          0.0                0.0             0.0   \n",
       "6          0.0  ...          0.0                0.0             0.0   \n",
       "7          0.0  ...          0.0                0.0             0.0   \n",
       "8          0.0  ...          0.0                0.0             0.0   \n",
       "9          0.0  ...          0.0                0.0             0.0   \n",
       "\n",
       "   spending scores  supervised learning  train machine  training data  \\\n",
       "0              0.0                  0.0       0.000000       0.000000   \n",
       "1              0.0                  0.0       0.000000       0.411714   \n",
       "2              0.0                  0.0       0.297392       0.257343   \n",
       "3              0.0                  0.0       0.000000       0.000000   \n",
       "4              0.0                  0.0       0.000000       0.000000   \n",
       "5              0.0                  0.0       0.000000       0.000000   \n",
       "6              0.0                  0.0       0.000000       0.000000   \n",
       "7              0.0                  0.0       0.000000       0.000000   \n",
       "8              0.0                  0.0       0.000000       0.000000   \n",
       "9              0.0                  0.0       0.000000       0.000000   \n",
       "\n",
       "   unsupervised learning  use following  versicolor virginica  \n",
       "0                    0.0            0.0                   0.0  \n",
       "1                    0.0            0.0                   0.0  \n",
       "2                    0.0            0.0                   0.0  \n",
       "3                    0.0            0.0                   0.0  \n",
       "4                    0.0            0.0                   0.0  \n",
       "5                    0.0            0.0                   0.0  \n",
       "6                    0.0            0.0                   0.0  \n",
       "7                    0.0            0.0                   0.0  \n",
       "8                    0.0            0.0                   0.0  \n",
       "9                    0.0            0.0                   0.0  \n",
       "\n",
       "[10 rows x 53 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2, 2), min_df=0.025, max_df=0.5, stop_words='english')\n",
    "word_matrix = vectorizer.fit_transform(df[0])\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "wm_df = pd.DataFrame(data=word_matrix.toarray(), columns=feature_names)\n",
    "wm_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the sparse word matrix into a coordinate matrix that includes only non-zero values (weights) and the rows and columns in which they appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t1.0\n",
      "  (1, 49)\t0.41171364214971173\n",
      "  (1, 28)\t0.5266452223745365\n",
      "  (1, 3)\t0.5266452223745365\n",
      "  (1, 21)\t0.43970283823932543\n",
      "  (1, 25)\t0.28712873491214824\n",
      "  (2, 20)\t0.27483761002183926\n",
      "  (2, 22)\t0.26560327297719427\n",
      "  (2, 48)\t0.29739224366595657\n",
      "  (2, 49)\t0.25734287700964276\n",
      "  (2, 21)\t0.27483761002183926\n",
      "  (2, 25)\t0.7178827918222094\n",
      "  (2, 4)\t0.3291812143547189\n",
      "  (3, 25)\t1.0\n",
      "  (4, 26)\t0.6079427215514522\n",
      "  (4, 25)\t0.7939808859869445\n",
      "  (5, 6)\t0.4028697432073993\n",
      "  (5, 18)\t0.38145874437527655\n",
      "  (5, 10)\t0.4028697432073993\n",
      "  (5, 0)\t0.7279293690706851\n",
      "  (6, 30)\t0.5768531568260111\n",
      "  (6, 37)\t0.5211465511802166\n",
      "  (6, 25)\t0.6290045370685583\n",
      "  (7, 30)\t0.7420272038414925\n",
      "  (7, 0)\t0.6703697701710424\n",
      "  :\t:\n",
      "  (91, 40)\t1.0\n",
      "  (93, 49)\t0.3686419838367719\n",
      "  (93, 28)\t0.47154993101664205\n",
      "  (93, 3)\t0.47154993101664205\n",
      "  (93, 21)\t0.39370307415818434\n",
      "  (93, 25)\t0.514181195949137\n",
      "  (94, 7)\t0.25937135995965965\n",
      "  (94, 19)\t0.46864782871828475\n",
      "  (94, 47)\t0.39376023290373574\n",
      "  (94, 50)\t0.19150502328554675\n",
      "  (94, 23)\t0.6278294062758754\n",
      "  (94, 5)\t0.24558675593114754\n",
      "  (94, 26)\t0.21655246888606403\n",
      "  (94, 25)\t0.1414101320845122\n",
      "  (95, 32)\t0.46610115002835284\n",
      "  (95, 38)\t0.2981076149481205\n",
      "  (95, 40)\t0.28226432588728073\n",
      "  (95, 29)\t0.2262835108521731\n",
      "  (95, 47)\t0.4525670217043462\n",
      "  (95, 50)\t0.22010566529441705\n",
      "  (95, 20)\t0.4977877281500323\n",
      "  (95, 21)\t0.24889386407501615\n",
      "  (96, 17)\t0.7306029456939581\n",
      "  (96, 29)\t0.5545762379110629\n",
      "  (96, 25)\t0.3983271671474636\n"
     ]
    }
   ],
   "source": [
    "coo_matrix = word_matrix.tocoo()\n",
    "print(coo_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tuples from the column numbers and weights in the coordinate matrix. Then sort the tuples in descending order based on the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1.0) => mathematical models\n",
      "(25, 1.0) => scikit learn\n",
      "(25, 1.0) => scikit learn\n",
      "(25, 1.0) => scikit learn\n",
      "(25, 1.0) => scikit learn\n",
      "(50, 1.0) => setosa versicolor\n",
      "(12, 1.0) => label column\n",
      "(29, 1.0) => data points\n",
      "(29, 1.0) => data points\n",
      "(9, 1.0) => make predictions\n",
      "(47, 1.0) => sepal width\n",
      "(32, 1.0) => annual income\n",
      "(32, 1.0) => annual income\n",
      "(32, 1.0) => annual income\n",
      "(32, 1.0) => annual income\n",
      "(31, 1.0) => number clusters\n",
      "(27, 1.0) => following statements\n",
      "(36, 1.0) => spending scores\n",
      "(49, 1.0) => petal width\n",
      "(40, 1.0) => low spending\n",
      "(32, 0.8805448238444277) => annual income\n",
      "(37, 0.8561751021506758) => segment customers\n",
      "(25, 0.803856205496376) => scikit learn\n",
      "(25, 0.7939808859869445) => scikit learn\n",
      "(49, 0.7731187523324384) => petal width\n",
      "(39, 0.7676246648600459) => incomes low\n",
      "(2, 0.7500525583741241) => learning algorithms\n",
      "(32, 0.7441198535306921) => annual income\n",
      "(30, 0.7420272038414925) => right number\n",
      "(6, 0.7420272038414925) => train machine\n",
      "(9, 0.7399158628045803) => make predictions\n",
      "(13, 0.7376325237851317) => column contains\n",
      "(19, 0.7353775931681876) => unsupervised learning\n",
      "(37, 0.7344073797929055) => segment customers\n",
      "(17, 0.7306029456939581) => classification models\n",
      "(0, 0.7279293690706851) => cat dog\n",
      "(46, 0.7218931373084843) => sepal length\n",
      "(25, 0.7178827918222094) => scikit learn\n",
      "(29, 0.7168242079334923) => data points\n",
      "(29, 0.7168242079334923) => data points\n",
      "(23, 0.7084869306845735) => data data\n",
      "(33, 0.7071067811865476) => income spending\n",
      "(39, 0.7071067811865476) => incomes low\n",
      "(33, 0.7071067811865476) => income spending\n",
      "(39, 0.7071067811865476) => incomes low\n",
      "(1, 0.7071067811865476) => machine learning\n",
      "(41, 0.7071067811865476) => add column\n",
      "(5, 0.7037578744864946) => training data\n",
      "(50, 0.6972539386195831) => setosa versicolor\n",
      "(50, 0.6972539386195831) => setosa versicolor\n",
      "(50, 0.6939068212915545) => setosa versicolor\n",
      "(25, 0.6845742955493805) => scikit learn\n",
      "(48, 0.6801256079913934) => petal length\n",
      "(20, 0.6787089217814357) => supervised learning\n",
      "(51, 0.6752023843666262) => versicolor virginica\n",
      "(29, 0.6726994246839768) => data points\n",
      "(0, 0.6703697701710424) => cat dog\n",
      "(0, 0.6703697701710424) => cat dog\n",
      "(9, 0.6613782273982346) => make predictions\n",
      "(37, 0.6503674936951619) => segment customers\n",
      "(0, 0.6503674936951619) => cat dog\n",
      "(9, 0.6408996597740573) => make predictions\n",
      "(29, 0.6373134305385414) => data points\n",
      "(17, 0.6359354687693229) => classification models\n",
      "(25, 0.6290045370685583) => scikit learn\n",
      "(23, 0.6278294062758754) => data data\n",
      "(41, 0.6241007262832584) => add column\n",
      "(50, 0.619913912867442) => setosa versicolor\n",
      "(26, 0.6079427215514522) => jupyter notebook\n",
      "(22, 0.6074253494015961) => credit card\n",
      "(20, 0.6026409911330227) => supervised learning\n",
      "(13, 0.6021379106039388) => column contains\n",
      "(27, 0.6014439028675274) => following statements\n",
      "(23, 0.594823672095326) => data data\n",
      "(45, 0.5875716412871428) => nearest neighbors\n",
      "(15, 0.5875716412871428) => millions rows\n",
      "(45, 0.5875716412871428) => nearest neighbors\n",
      "(15, 0.5875716412871428) => millions rows\n",
      "(46, 0.5851682335794984) => sepal length\n",
      "(12, 0.5851682335794984) => label column\n",
      "(30, 0.5768531568260111) => right number\n",
      "(31, 0.5657375217907783) => number clusters\n",
      "(47, 0.5656508960478019) => sepal width\n",
      "(32, 0.564279999559708) => annual income\n",
      "(49, 0.564279999559708) => petal width\n",
      "(12, 0.5638314313493284) => label column\n",
      "(51, 0.5613878132083908) => versicolor virginica\n",
      "(11, 0.5595611884647269) => data scientists\n",
      "(30, 0.5595611884647269) => right number\n",
      "(25, 0.5578013866175753) => scikit learn\n",
      "(2, 0.5563444371163124) => learning algorithms\n",
      "(2, 0.5563444371163124) => learning algorithms\n",
      "(29, 0.5545762379110629) => data points\n",
      "(40, 0.5497781594090745) => low spending\n",
      "(48, 0.5433625732975231) => petal length\n",
      "(51, 0.5409181088439147) => versicolor virginica\n",
      "(18, 0.5298226336223203) => learning models\n",
      "(28, 0.5266452223745365) => following code\n",
      "(3, 0.5266452223745365) => build mathematical\n",
      "(31, 0.526231321643103) => number clusters\n",
      "(8, 0.526231321643103) => learning algorithm\n",
      "(11, 0.526231321643103) => data scientists\n",
      "(21, 0.524171258635692) => labeled data\n",
      "(37, 0.5211465511802166) => segment customers\n",
      "(25, 0.5166857792288073) => scikit learn\n",
      "(25, 0.514181195949137) => scikit learn\n",
      "(11, 0.5134579738259717) => data scientists\n",
      "(6, 0.5134579738259717) => train machine\n",
      "(41, 0.5115535290919585) => add column\n",
      "(37, 0.5111043510219243) => segment customers\n",
      "(45, 0.5065949667294068) => nearest neighbors\n",
      "(15, 0.5065949667294068) => millions rows\n",
      "(33, 0.5065949667294068) => income spending\n",
      "(23, 0.506559498473125) => data data\n",
      "(38, 0.5018640752209794) => high incomes\n",
      "(10, 0.5018640752209794) => 1s 0s\n",
      "(35, 0.5) => use following\n",
      "(34, 0.5) => spending score\n",
      "(43, 0.5) => making predictions\n",
      "(42, 0.5) => predict class\n",
      "(47, 0.4998901342511416) => sepal width\n",
      "(20, 0.4977877281500323) => supervised learning\n",
      "(5, 0.4861696299796273) => training data\n",
      "(18, 0.4861696299796273) => learning models\n",
      "(22, 0.48528124372751885) => credit card\n",
      "(29, 0.4827173253309481) => data points\n",
      "(2, 0.4796713656799762) => learning algorithms\n",
      "(26, 0.47234149947909015) => jupyter notebook\n",
      "(28, 0.47154993101664205) => following code\n",
      "(3, 0.47154993101664205) => build mathematical\n",
      "(8, 0.46990818526810524) => learning algorithm\n",
      "(19, 0.46864782871828475) => unsupervised learning\n",
      "(22, 0.46849281984084057) => credit card\n",
      "(32, 0.46610115002835284) => annual income\n",
      "(46, 0.4621528967566216) => sepal length\n",
      "(0, 0.4621528967566216) => cat dog\n",
      "(25, 0.45775357113688714) => scikit learn\n",
      "(10, 0.45593647959569067) => 1s 0s\n",
      "(28, 0.45593647959569067) => following code\n",
      "(3, 0.45593647959569067) => build mathematical\n",
      "(47, 0.4525670217043462) => sepal width\n",
      "(49, 0.4422746959462308) => petal width\n",
      "(29, 0.44074195954401685) => data points\n",
      "(21, 0.43970283823932543) => labeled data\n",
      "(24, 0.4309876875284723) => means clustering\n",
      "(16, 0.4309876875284723) => datasets like\n",
      "(14, 0.4309876875284723) => real world\n",
      "(50, 0.42870910860126965) => setosa versicolor\n",
      "(25, 0.42717466671388643) => scikit learn\n",
      "(20, 0.4190127412341422) => supervised learning\n",
      "(21, 0.4190127412341422) => labeled data\n",
      "(20, 0.4128385301026845) => supervised learning\n",
      "(36, 0.4119067757311599) => spending scores\n",
      "(49, 0.41171364214971173) => petal width\n",
      "(49, 0.4113900683843245) => petal width\n",
      "(25, 0.4104431778683697) => scikit learn\n",
      "(8, 0.40699193453118504) => learning algorithm\n",
      "(25, 0.4062815959548538) => scikit learn\n",
      "(6, 0.4028697432073993) => train machine\n",
      "(10, 0.4028697432073993) => 1s 0s\n",
      "(24, 0.39952901367606747) => means clustering\n",
      "(16, 0.39952901367606747) => datasets like\n",
      "(14, 0.39952901367606747) => real world\n",
      "(22, 0.3989674659070634) => credit card\n",
      "(25, 0.3983271671474636) => scikit learn\n",
      "(9, 0.39735337634100004) => make predictions\n",
      "(21, 0.39735337634100004) => labeled data\n",
      "(47, 0.39376023290373574) => sepal width\n",
      "(21, 0.39370307415818434) => labeled data\n",
      "(1, 0.3933255742665142) => machine learning\n",
      "(24, 0.3933255742665142) => means clustering\n",
      "(16, 0.3933255742665142) => datasets like\n",
      "(14, 0.3933255742665142) => real world\n",
      "(25, 0.3924847083509714) => scikit learn\n",
      "(26, 0.39233235961520574) => jupyter notebook\n",
      "(27, 0.3917571980615879) => following statements\n",
      "(52, 0.389821556236359) => model accuracy\n",
      "(44, 0.389821556236359) => regression classification\n",
      "(1, 0.389821556236359) => machine learning\n",
      "(17, 0.389821556236359) => classification models\n",
      "(46, 0.38936728402848236) => sepal length\n",
      "(12, 0.38936728402848236) => label column\n",
      "(18, 0.38145874437527655) => learning models\n",
      "(47, 0.3809482187543103) => sepal width\n",
      "(9, 0.38066720368441453) => make predictions\n",
      "(23, 0.37915028732924827) => data data\n",
      "(51, 0.37354394099375515) => versicolor virginica\n",
      "(7, 0.37259639219632973) => learning model\n",
      "(13, 0.37242181181412043) => column contains\n",
      "(4, 0.3716296432513177) => mathematical models\n",
      "(49, 0.3686419838367719) => petal width\n",
      "(26, 0.3665595920155999) => jupyter notebook\n",
      "(47, 0.36125646319792853) => sepal width\n",
      "(18, 0.3554970719708412) => learning models\n",
      "(46, 0.3553421942731911) => sepal length\n",
      "(36, 0.35392527587640576) => spending scores\n",
      "(19, 0.35392527587640576) => unsupervised learning\n",
      "(48, 0.35392527587640576) => petal length\n",
      "(12, 0.3521765586342421) => label column\n",
      "(48, 0.3521765586342421) => petal length\n",
      "(51, 0.3409015834018214) => versicolor virginica\n",
      "(19, 0.3366148255887518) => unsupervised learning\n",
      "(36, 0.3357414354155537) => spending scores\n",
      "(52, 0.3303913695476211) => model accuracy\n",
      "(44, 0.3303913695476211) => regression classification\n",
      "(35, 0.3303913695476211) => use following\n",
      "(34, 0.3303913695476211) => spending score\n",
      "(43, 0.3303913695476211) => making predictions\n",
      "(42, 0.3303913695476211) => predict class\n",
      "(4, 0.3291812143547189) => mathematical models\n",
      "(22, 0.32838565863700264) => credit card\n",
      "(25, 0.32790922544088724) => scikit learn\n",
      "(25, 0.31656512520167623) => scikit learn\n",
      "(23, 0.31609335369122366) => data data\n",
      "(22, 0.31609335369122366) => credit card\n",
      "(52, 0.3146380988467465) => model accuracy\n",
      "(44, 0.3146380988467465) => regression classification\n",
      "(35, 0.3146380988467465) => use following\n",
      "(34, 0.3146380988467465) => spending score\n",
      "(43, 0.3146380988467465) => making predictions\n",
      "(42, 0.3146380988467465) => predict class\n",
      "(38, 0.3146380988467465) => high incomes\n",
      "(22, 0.31453156103212515) => credit card\n",
      "(9, 0.3134687305313628) => make predictions\n",
      "(13, 0.31283232137684347) => column contains\n",
      "(26, 0.31108549780016076) => jupyter notebook\n",
      "(47, 0.3089339527615277) => sepal width\n",
      "(25, 0.30507462960625437) => scikit learn\n",
      "(23, 0.3006332589811738) => data data\n",
      "(50, 0.30049963847799754) => setosa versicolor\n",
      "(23, 0.29985322757978966) => data data\n",
      "(36, 0.2984855343894466) => spending scores\n",
      "(38, 0.2981076149481205) => high incomes\n",
      "(40, 0.2979162772641291) => low spending\n",
      "(5, 0.2979162772641291) => training data\n",
      "(48, 0.29739224366595657) => petal length\n",
      "(47, 0.29736977431594175) => sepal width\n",
      "(49, 0.290527640721641) => petal width\n",
      "(25, 0.28712873491214824) => scikit learn\n",
      "(51, 0.28635549882846556) => versicolor virginica\n",
      "(40, 0.28226432588728073) => low spending\n",
      "(47, 0.28209162126329296) => sepal width\n",
      "(26, 0.27584798408472355) => jupyter notebook\n",
      "(50, 0.27510393119258275) => setosa versicolor\n",
      "(20, 0.27483761002183926) => supervised learning\n",
      "(21, 0.27483761002183926) => labeled data\n",
      "(25, 0.26958617175642563) => scikit learn\n",
      "(22, 0.26560327297719427) => credit card\n",
      "(20, 0.26269537670418824) => supervised learning\n",
      "(7, 0.25937135995965965) => learning model\n",
      "(49, 0.25734287700964276) => petal width\n",
      "(21, 0.24889386407501615) => labeled data\n",
      "(25, 0.24857809209082235) => scikit learn\n",
      "(32, 0.24597355512157087) => annual income\n",
      "(5, 0.24558675593114754) => training data\n",
      "(29, 0.2262835108521731) => data points\n",
      "(50, 0.22010566529441705) => setosa versicolor\n",
      "(7, 0.21951968586736945) => learning model\n",
      "(26, 0.21655246888606403) => jupyter notebook\n",
      "(25, 0.21253201496629093) => scikit learn\n",
      "(19, 0.1983207092692564) => unsupervised learning\n",
      "(50, 0.19150502328554675) => setosa versicolor\n",
      "(50, 0.16208081940968688) => setosa versicolor\n",
      "(25, 0.1414101320845122) => scikit learn\n"
     ]
    }
   ],
   "source": [
    "tuples = list(zip(coo_matrix.col, coo_matrix.data))\n",
    "sorted_tuples = sorted(tuples, key=lambda x: x[1], reverse=True)\n",
    "vocab = [key for key, _ in vectorizer.vocabulary_.items()]\n",
    "\n",
    "for _, tuple in enumerate(sorted_tuples):\n",
    "    print(f'{tuple} => {vocab[tuple[0]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the top keywords by weight and use `set` to eliminate duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scikit learn', 'mathematical models'}\n"
     ]
    }
   ],
   "source": [
    "keywords = []\n",
    "num_keywords = 5\n",
    "\n",
    "for tuple in sorted_tuples[:num_keywords]:\n",
    "    keywords.append(vocab[tuple[0]])\n",
    "    \n",
    "print(set(keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keyword extraction sometimes works better when you sum all the values for a given word and select the words yielding the highest sums rather than the words with the highest individual values. Sort keywords based on that criterion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine learning             13.819216\n",
      "nearest neighbors             6.901019\n",
      "means clustering              6.447980\n",
      "unsupervised learning         5.286333\n",
      "supervised learning           4.542468\n",
      "training data                 4.519290\n",
      "data points                   4.133683\n",
      "learning models               3.733430\n",
      "real world                    3.273201\n",
      "learning model                3.184781\n",
      "1s 0s                         3.181189\n",
      "learning algorithm            3.148522\n",
      "following code                2.890544\n",
      "use following                 2.778309\n",
      "learning algorithms           2.697675\n",
      "make predictions              2.642662\n",
      "spending scores               2.513924\n",
      "predict class                 2.400059\n",
      "annual income                 2.342413\n",
      "train machine                 2.226982\n",
      "right number                  2.181838\n",
      "scikit learn                  2.129959\n",
      "labeled data                  2.092886\n",
      "model accuracy                2.091969\n",
      "following statements          2.025025\n",
      "making predictions            1.993201\n",
      "number clusters               1.920809\n",
      "millions rows                 1.878442\n",
      "segment customers             1.842761\n",
      "jupyter notebook              1.756360\n",
      "label column                  1.752948\n",
      "classification models         1.733431\n",
      "cat dog                       1.700811\n",
      "spending score                1.681738\n",
      "income spending               1.681738\n",
      "column contains               1.658355\n",
      "datasets like                 1.599250\n",
      "add column                    1.490254\n",
      "mathematical models           1.454132\n",
      "build mathematical            1.454132\n",
      "data data                     1.403131\n",
      "data scientists               1.360670\n",
      "high incomes                  1.223842\n",
      "low spending                  1.223842\n",
      "incomes low                   1.223842\n",
      "petal length                  1.145029\n",
      "sepal width                   1.145029\n",
      "sepal length                  1.145029\n",
      "petal width                   1.145029\n",
      "regression classification     1.114610\n",
      "setosa versicolor             1.034851\n",
      "versicolor virginica          1.034851\n",
      "credit card                   0.851487\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "summed_weights = pd.Series(dtype='float32')\n",
    "\n",
    "for col_name, col_data in wm_df.iteritems():\n",
    "    summed_weights = summed_weights.append(pd.Series({ col_name: np.sum(col_data) }))\n",
    "    \n",
    "sorted_summed_weights = summed_weights.sort_values(ascending=False)\n",
    "print(sorted_summed_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the top keywords by summed weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machine learning', 'nearest neighbors', 'means clustering', 'unsupervised learning', 'supervised learning']\n"
     ]
    }
   ],
   "source": [
    "keywords = []\n",
    "\n",
    "for idx, _ in sorted_summed_weights[:num_keywords].items():\n",
    "    keywords.append(idx)\n",
    "    \n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you read **chapter-1.txt**, you'll see that these keywords highlight some of the most important concepts introduced in the chapter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
